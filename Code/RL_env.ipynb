{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class LiquidationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_assets=3, initial_shares=100, price_std=0.1, max_steps=100):\n",
    "        super(LiquidationEnv, self).__init__()\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.n_assets = n_assets\n",
    "        self.initial_shares = np.full(n_assets, initial_shares, dtype=np.float32)\n",
    "        self.price_std = price_std\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=initial_shares,\n",
    "            shape=(n_assets,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"prices\": spaces.Box(low=0, high=np.inf, shape=(n_assets,), dtype=np.float32),\n",
    "            \"remaining\": spaces.Box(low=0, high=initial_shares, shape=(n_assets,), dtype=np.float32),\n",
    "            \"acc_revenue\": spaces.Box(low=-np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.current_step = 0\n",
    "        self.reset()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"prices\": self.state['prices'].copy(),\n",
    "            \"remaining\": self.state['remaining'].copy(),\n",
    "            \"acc_revenue\": np.array([self.state['acc_revenue']], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "    def _next_price(self):\n",
    "        # Simple random walk price model (customize this with your own price model)\n",
    "        return self.state['prices'] * np.exp(np.random.normal(0, self.price_std, self.n_assets))\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset initial prices (customize with your price initialization)\n",
    "        self.state = {\n",
    "            'prices': np.abs(np.random.normal(100, 10, self.n_assets)).astype(np.float32),\n",
    "            'remaining': self.initial_shares.copy(),\n",
    "            'acc_revenue': 0.0\n",
    "        }\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Clip actions to valid range\n",
    "        action = np.clip(action, 0, self.state['remaining'])\n",
    "        \n",
    "        # Calculate revenue from current prices\n",
    "        step_revenue = np.sum(action * self.state['prices'])\n",
    "        \n",
    "        # Update state\n",
    "        self.state['remaining'] -= action\n",
    "        self.state['acc_revenue'] += step_revenue\n",
    "        \n",
    "        # Update prices for next step\n",
    "        self.state['prices'] = self._next_price()\n",
    "        \n",
    "        # Update step counter\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check termination conditions\n",
    "        done = (np.sum(self.state['remaining']) <= 0) or (self.current_step >= self.max_step)\n",
    "        \n",
    "        # Reward is the immediate revenue gained\n",
    "        reward = step_revenue\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Prices: {self.state['prices']}\")\n",
    "        print(f\"Remaining: {self.state['remaining']}\")\n",
    "        print(f\"Accumulated Revenue: {self.state['acc_revenue']:.2f}\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Create environment\n",
    "env = LiquidationEnv(n_assets=3, initial_shares=100)\n",
    "\n",
    "# Verify environment compatibility\n",
    "check_env(env)\n",
    "\n",
    "# Create and train model\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Test trained model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_shrink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
