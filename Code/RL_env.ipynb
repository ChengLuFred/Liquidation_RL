{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PriceModel():\n",
    "    '''\n",
    "    Note:This is a stateless class, gathering price evolution models in one place\n",
    "    '''\n",
    "    def price_model_1(current_price, current_action, tau, vol_matrix, perm_impact_matrix, random_vector):\n",
    "        return current_price + np.sqrt(tau) * (vol_matrix@random_vector) - perm_impact_matrix @ current_action\n",
    "    def price_model_2(current_price):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class LiquidationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_assets=3, \n",
    "                 initial_shares=100, \n",
    "                 initial_prices=50, \n",
    "                 max_steps = 5,\n",
    "                 price_model=PriceModel,\n",
    "                 tau = 1,\n",
    "                 temp_price_matrix = np.identity(3)*0.1,\n",
    "                 vol_matrix = np.identity(3)*.4,\n",
    "                 perm_impact_matrix = np.identity(3)*0.052\n",
    "                 ):\n",
    "        super(LiquidationEnv, self).__init__()\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.n_assets = n_assets\n",
    "        self.initial_shares = np.full(n_assets, initial_shares, dtype=np.float32)\n",
    "        self.initial_prices = np.full(n_assets, initial_prices, dtype=np.float32)\n",
    "        self.max_steps = max_steps\n",
    "        self.price_generator = price_model.price_model_1\n",
    "        self.temp_price_matrix = temp_price_matrix\n",
    "        self.tau = tau\n",
    "        self.vol_matrix = vol_matrix\n",
    "        self.perm_impact_matrix = perm_impact_matrix\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high= 1,\n",
    "            shape=(n_assets,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"prices\": spaces.Box(low = -np.inf, high=np.inf, shape=(n_assets,), dtype=np.float32),\n",
    "            \"remaining\": spaces.Box(low = 0, high=1, shape=(n_assets,), dtype=np.float32)\n",
    "            #\"acc_revenue\": spaces.Box(low = -np.inf, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.current_step = 0\n",
    "        self.reset()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"prices\": self.state['prices'].copy().astype(np.float32)/self.initial_prices,\n",
    "            \"remaining\": self.state['remaining'].copy().astype(np.float32)/self.initial_shares\n",
    "            #\"acc_revenue\": np.array([self.state['acc_revenue']], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "    def _next_price(self, current_price , current_action, tau, vol_matrix, perm_impact_matrix, random_vector):\n",
    "        # actual_action = self.state['remaining'] * current_action\n",
    "        return self.price_generator(current_price, current_action, tau, vol_matrix, perm_impact_matrix, random_vector)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset initial prices (customize with your price initialization)\n",
    "        self.state = {\n",
    "            'prices': self.initial_prices.copy()/self.initial_prices,\n",
    "            'remaining': self.initial_shares.copy()/self.initial_shares\n",
    "            #'acc_revenue': 0.0\n",
    "        }\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_reward(self, state, action, temp_price_matrix):\n",
    "        '''\n",
    "        The function to calculate the reward\n",
    "        '''\n",
    "        # actual_action = action * state['remaining']\n",
    "        reward = action.dot(state['prices']*self.initial_prices - temp_price_matrix.dot(action))/(np.dot(self.initial_prices, self.initial_shares))\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: need a better way than clipping\n",
    "        if self.current_step == self.max_steps - 1:\n",
    "            actual_action = self.state['remaining']*self.initial_shares\n",
    "        else:\n",
    "            actual_action = np.minimum(self.state['remaining']*self.initial_shares, action * self.initial_shares)\n",
    "        \n",
    "        reward = self._get_reward(self.state, actual_action, self.temp_price_matrix)\n",
    "        # Update state\n",
    "        self.state['remaining'] = (self.state['remaining']*self.initial_shares - actual_action)/self.initial_shares\n",
    "        # step_revenue = np.sum(actual_action * (self.state['prices'] - self.temp_price_matrix.dot(actual_action))) # Calculate revenue from current prices\n",
    "        # self.state['acc_revenue'] += step_revenue # TODO: what's the third part of the state? what's the formula to calculate it?\n",
    "\n",
    "        random_vector = np.random.normal(size = self.n_assets)\n",
    "        self.state['prices'] = self._next_price(self.state['prices']*self.initial_prices , actual_action, self.tau, self.vol_matrix, self.perm_impact_matrix, random_vector)/self.initial_prices\n",
    "        \n",
    "        # Update step counter\n",
    "        \n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check termination conditions\n",
    "        done = (np.sum(self.state['remaining']) <= 0) or (self.current_step >= self.max_steps)\n",
    "            \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Prices: {self.state['prices']}\")\n",
    "        print(f\"Remaining: {self.state['remaining']}\")\n",
    "        #print(f\"Accumulated Revenue: {self.state['acc_revenue']:.2f}\\n\")\n",
    "        \n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruijing/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:361: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "env = LiquidationEnv(n_assets=3, initial_shares=100)\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Prices: [1. 1. 1.]\n",
      "Remaining: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 6\n",
      "Prices: [0.86499436 0.89342401 0.88711833]\n",
      "Remaining: [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "action = np.ones(1)*0.1\n",
    "env.step(action)\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a7b2912acf4e6cbfe94cd3ef35973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/ruijing/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: \n",
       "UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting \n",
       "modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first\n",
       "with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/ruijing/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: \n",
       "UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting \n",
       "modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first\n",
       "with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(env_eval, \n\u001b[1;32m      9\u001b[0m                             best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../log/model/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m                             eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     11\u001b[0m                             deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     12\u001b[0m                             render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m                             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9999999\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../log/tensorboard_test/\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8e5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:307\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    299\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    305\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:269\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mrecord(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/total_timesteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, exclude\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdump(step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py:208\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mreset_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 208\u001b[0m values, log_prob, entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollout_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/policies.py:708\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    706\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[1;32m    707\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m--> 708\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[1;32m    710\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:175\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.log_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    Get the log probabilities of actions according to the distribution.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    Note that you must first call the ``proba_distribution()`` method.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sum_independent_dims(log_prob)\n",
      "File \u001b[0;32m~/ruijing/liquidation_RL/rl_env/lib/python3.9/site-packages/torch/distributions/normal.py:89\u001b[0m, in \u001b[0;36mNormal.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     84\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     85\u001b[0m log_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     86\u001b[0m     math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, Real) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\u001b[38;5;241m.\u001b[39mlog()\n\u001b[1;32m     87\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;241m-\u001b[39m((value \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m)\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;241m-\u001b[39m log_scale\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;241m-\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi))\n\u001b[1;32m     92\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "env = LiquidationEnv(n_assets=3, initial_shares = 100)\n",
    "env_eval = LiquidationEnv(n_assets=3, initial_shares=100)\n",
    "eval_callback = EvalCallback(env_eval, \n",
    "                            best_model_save_path=f\"../log/model/test/\",\n",
    "                            eval_freq=100,\n",
    "                            deterministic=True, \n",
    "                            render=False,\n",
    "                            verbose=0)\n",
    "model = PPO(\"MultiInputPolicy\", env, gamma=0.9999999, tensorboard_log=\"../log/tensorboard_test/\", verbose=0)\n",
    "model.learn(int(8e5), progress_bar=True, callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26283311456a41c1bc2d3006367a0643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7c727b4c9550>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = LiquidationEnv(n_assets=5, initial_shares = 100, \n",
    "                     vol_matrix= np.identity(5),\n",
    "                     perm_impact_matrix= 0.05* np.identity(5), temp_price_matrix= 0.1*np.identity(5))\n",
    "env_eval = LiquidationEnv(n_assets=5, initial_shares =  100, \n",
    "                     vol_matrix= np.identity(5),\n",
    "                     perm_impact_matrix= 0.05* np.identity(5), temp_price_matrix= 0.1*np.identity(5))\n",
    "eval_callback = EvalCallback(env_eval, \n",
    "                            best_model_save_path=f\"../log/model/test/\",\n",
    "                            eval_freq=100,\n",
    "                            deterministic=True, \n",
    "                            render=False,\n",
    "                            verbose=0)\n",
    "model = PPO(\"MultiInputPolicy\", env, gamma=0.9999999999, tensorboard_log=\"../log/tensorboard_test/\", verbose=0)\n",
    "model.learn(int(8e5), progress_bar=True, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Load the best model from EvalCallback's auto-saved checkpoints\n",
    "best_model = PPO.load(\"../log/model/test/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liquidation_actions(model, env, n_episodes=5):\n",
    "    all_episode_actions = []\n",
    "    all_remaining_shares = []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_actions = []\n",
    "        remaining_shares = [env.state['remaining'].copy()]\n",
    "        prices = [env.initial_prices.copy()]\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            episode_actions.append(action)\n",
    "            obs, _, done, _ = env.step(action)\n",
    "            remaining_shares.append(env.state['remaining'].copy())\n",
    "            prices.append(env.state['prices'].copy())\n",
    "        all_episode_actions.append(episode_actions)\n",
    "        all_remaining_shares.append(np.array(remaining_shares))\n",
    "    average_actions = np.mean(all_episode_actions, axis=0)\n",
    "    average_remaining = np.mean(all_remaining_shares, axis =0)\n",
    "    return average_actions, average_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_actions, remaining_shares = get_liquidation_actions(\n",
    "    best_model, \n",
    "    env_eval,  # Use your evaluation environment\n",
    "    n_episodes=50# For example, get one episode sample\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17651263, 0.18419257, 0.18629867, 0.18166551, 0.19136107],\n",
       "       [0.18256962, 0.19360909, 0.19116244, 0.18924311, 0.20006947],\n",
       "       [0.18884541, 0.2033678 , 0.19620436, 0.19709718, 0.20909494],\n",
       "       [0.19533601, 0.21347328, 0.20140977, 0.20522076, 0.21843283],\n",
       "       [0.20203862, 0.22391996, 0.20677942, 0.21361296, 0.22807972]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100.      , 100.      , 100.      , 100.      , 100.      ],\n",
       "       [ 82.34875 ,  81.58076 ,  81.37013 ,  81.83344 ,  80.86389 ],\n",
       "       [ 64.095146,  62.22479 ,  62.25592 ,  62.91235 ,  60.860687],\n",
       "       [ 45.209526,  41.88778 ,  42.6326  ,  43.20042 ,  39.948895],\n",
       "       [ 25.663488,  20.525743,  22.477911,  22.662226,  18.088137],\n",
       "       [  0.      ,   0.      ,   0.      ,   0.      ,   0.      ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining_shares*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Optimal Liquidation Strategy ===\n",
      "Step 1:\n",
      "  Action (% to liquidate): 0.17651253938674927\n",
      "  Remaining shares: 1.0\n",
      "\n",
      "Step 2:\n",
      "  Action (% to liquidate): 0.18419243395328522\n",
      "  Remaining shares: 1.0\n",
      "\n",
      "Step 3:\n",
      "  Action (% to liquidate): 0.18629863858222961\n",
      "  Remaining shares: 1.0\n",
      "\n",
      "Step 4:\n",
      "  Action (% to liquidate): 0.1816655993461609\n",
      "  Remaining shares: 1.0\n",
      "\n",
      "Final remaining shares: 1.0 (Should be all zeros)\n"
     ]
    }
   ],
   "source": [
    "# Get liquidation trajectories\n",
    "episode_actions, remaining_shares = get_liquidation_actions(\n",
    "    best_model, \n",
    "    env_eval,  # Use your evaluation env\n",
    "    n_episodes= 1  # Get 5 episode samples\n",
    ")\n",
    "\n",
    "# Inspect first episode\n",
    "print(\"=== Optimal Liquidation Strategy ===\")\n",
    "for step, (action, remaining) in enumerate(zip(episode_actions[0], remaining_shares[0][:-1])):\n",
    "    print(f\"Step {step+1}:\")\n",
    "    print(f\"  Action (% to liquidate): {action}\")\n",
    "    print(f\"  Remaining shares: {remaining}\\n\")\n",
    "\n",
    "# Verify full liquidation at last step\n",
    "final_remaining = remaining_shares[0][-1]\n",
    "print(f\"Final remaining shares: {final_remaining} (Should be all zeros)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4336197], dtype=float32)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# action = 0.5\n",
    "# obs, _, done, _ = env.step(action)\n",
    "\n",
    "obs['remaining'] = np.array([59], dtype=np.float32)\n",
    "action, _ = model.predict(obs, deterministic=True)\n",
    "action\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[235], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mobs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obs' is not defined"
     ]
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6569459], dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
