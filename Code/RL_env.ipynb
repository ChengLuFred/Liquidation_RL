{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PriceModel():\n",
    "    '''\n",
    "    Note:This is a stateless class, gathering price evolution models in one place\n",
    "    '''\n",
    "    # TODO: write the concrete price model\n",
    "    def price_model_1(current_price, current_action, tau, vol_matrix, perm_impact_matrix, random_vector):\n",
    "        return current_price + tau**0.5 * vol_matrix.dot(random_vector) - perm_impact_matrix.dot(current_action) \n",
    "    def price_model_2(current_price):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pandas as pd\n",
    "\n",
    "class LiquidationEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_assets=3, \n",
    "                 initial_shares=100, \n",
    "                 initial_prices=100, \n",
    "                 max_steps=5,\n",
    "                 price_model=PriceModel):\n",
    "        super(LiquidationEnv, self).__init__()\n",
    "        \n",
    "        # Environment parameters\n",
    "        self.n_assets = n_assets\n",
    "        self.initial_shares = np.full(n_assets, initial_shares, dtype=np.float32)\n",
    "        self.initial_prices = np.full(n_assets, initial_prices, dtype=np.float32)\n",
    "        self.max_steps = max_steps\n",
    "        self.price_generator = price_model.price_model_1\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Box(\n",
    "            low=0,\n",
    "            high= 1,\n",
    "            shape=(n_assets,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"prices\": spaces.Box(low = -np.inf, high=np.inf, shape=(n_assets,), dtype=np.float32),\n",
    "            \"remaining\": spaces.Box(low = 0, high=initial_shares, shape=(n_assets,), dtype=np.float32),\n",
    "            \"acc_revenue\": spaces.Box(low = 0, high=np.inf, shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.current_step = 0\n",
    "        self.reset()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"prices\": self.state['prices'].copy(),\n",
    "            \"remaining\": self.state['remaining'].copy(),\n",
    "            \"acc_revenue\": np.array([self.state['acc_revenue']], dtype=np.float32)\n",
    "        }\n",
    "\n",
    "    def _next_price(self, current_price , current_action, tau, vol_matrix, perm_impact_matrix, random_vector):\n",
    "        # TODO: not sure do we need action value when calculating the next price\n",
    "        actual_action = self.state['remaining'] * current_action\n",
    "        return self.price_generator(current_price, actual_action, tau, vol_matrix, perm_impact_matrix, random_vector)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset initial prices (customize with your price initialization)\n",
    "        self.state = {\n",
    "            'prices': self.initial_prices.copy(),\n",
    "            'remaining': self.initial_shares.copy(),\n",
    "            'acc_revenue': 0.0\n",
    "        }\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_reward(self, state, action, temp_price_matrix):\n",
    "        '''\n",
    "        TODO: The function to calculate the reward\n",
    "        '''\n",
    "        actual_action = action * state['remaining']\n",
    "        reward = actual_action.dot(state['prices'] - temp_price_matrix.dot(actual_action))\n",
    "        return reward\n",
    "\n",
    "    def step(self, action, temp_price_matrix):\n",
    "        # TODO: need a better way than clipping\n",
    "        # action = np.clip(action, 0, self.state['remaining'])        # Clip actions to valid range\n",
    "        actual_action = self.state['remaining'] * action\n",
    "        # Update state\n",
    "        self.state['remaining'] -= actual_action\n",
    "        self.state['prices'] = self._next_price()\n",
    "        step_revenue = np.sum(actual_action * (self.state['prices'] - temp_price_matrix.dot(actual_action))) # Calculate revenue from current prices\n",
    "        self.state['acc_revenue'] += step_revenue # TODO: what's the third part of the state? what's the formulor to calculate it?\n",
    "        \n",
    "        # Update step counter\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Check termination conditions\n",
    "        done = (np.sum(self.state['remaining']) <= 0) or (self.current_step >= self.max_step)\n",
    "        if done:\n",
    "            reward = 0.\n",
    "        else:\n",
    "            reward = self._get_reward\n",
    "        \n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "        print(f\"Prices: {self.state['prices']}\")\n",
    "        print(f\"Remaining: {self.state['remaining']}\")\n",
    "        print(f\"Accumulated Revenue: {self.state['acc_revenue']:.2f}\\n\")\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheng/app/anaconda3/envs/rl_shrink/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py:385: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LiquidationEnv' object has no attribute 'max_step'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1681481/1703532053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Verify environment compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcheck_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Create and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/anaconda3/envs/rl_shrink/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;31m# ============ Check the returned values ===============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0m_check_returned_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/anaconda3/envs/rl_shrink/lib/python3.7/site-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[0;34m(env, observation_space, action_space)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Sample a random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The `step()` method must return four values: obs, reward, done, info\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1681481/3629978568.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Check termination conditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'remaining'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Reward is the immediate revenue gained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LiquidationEnv' object has no attribute 'max_step'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# Create environment\n",
    "env = LiquidationEnv(n_assets=3, initial_shares=100)\n",
    "\n",
    "# Verify environment compatibility\n",
    "check_env(env)\n",
    "\n",
    "# Create and train model\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=5)\n",
    "\n",
    "# Test trained model\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
